{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e80323",
   "metadata": {},
   "source": [
    "# Advance Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d175eef",
   "metadata": {},
   "source": [
    "#### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4a700b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baec81b",
   "metadata": {},
   "source": [
    "#### Read the data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51bc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('../data/original_raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c5b477f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>18556</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>RT @Yankees: Congrats to Derek Jeter, who will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>3046</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@Darth_Maxwell dude I was pissed but then look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>422</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\"@xoxoclauudia: YASSSSSS, ORDERED CHINESE &amp;#12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>5927</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@corey_emanuel bet.... Pay back is a bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>1608</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;#8220;@ToriJBerman: Shavonne was doing ballet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "18151       18556      3            0                   0        3      2   \n",
       "2974         3046      3            0                   3        0      1   \n",
       "416           422      3            1                   2        0      1   \n",
       "5763         5927      3            0                   2        1      1   \n",
       "1573         1608      3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \n",
       "18151  RT @Yankees: Congrats to Derek Jeter, who will...  \n",
       "2974   @Darth_Maxwell dude I was pissed but then look...  \n",
       "416    \"@xoxoclauudia: YASSSSSS, ORDERED CHINESE &#12...  \n",
       "5763          @corey_emanuel bet.... Pay back is a bitch  \n",
       "1573   &#8220;@ToriJBerman: Shavonne was doing ballet...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1ae0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'Unnamed: 0' column\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48acbaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7280</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@white_thunduh my bitch dont want your ugly as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bitches so stupid!!! Yal can't know how I'm bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10176</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I didn't forsake all other bitches for my wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>#RadioFreeNorthwest 218\\nCovington reads respo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@KatieFraaancis bitch nigga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "7280       3            2                   1        0      0   \n",
       "8090       6            2                   4        0      1   \n",
       "10176      3            2                   1        0      0   \n",
       "756        3            0                   1        2      2   \n",
       "3763       3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \n",
       "7280   @white_thunduh my bitch dont want your ugly as...  \n",
       "8090   Bitches so stupid!!! Yal can't know how I'm bo...  \n",
       "10176  I didn't forsake all other bitches for my wife...  \n",
       "756    #RadioFreeNorthwest 218\\nCovington reads respo...  \n",
       "3763                         @KatieFraaancis bitch nigga  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5aaecc",
   "metadata": {},
   "source": [
    "### Understanding the Features\n",
    "\n",
    "    1 - count (numeric - representative of the total number of people who rated this entry)\n",
    "    2 - hate_speech (numeric - from the total count above)\n",
    "    3 - offensive_language (numeric - from the total count above)\n",
    "    4 - neither (numeric - from the total count above)\n",
    "    5 - class (numeric 0 - offensive , 1 - hate speech or 2 - neither)\n",
    "    6 - tweet - (text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7d26613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer, stemmer and TweetTokenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba5889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove 'rt' as it refer to a re-tweet in the tweeet and is non important for our model\n",
    "stop_words.update(['RT', 'I'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ca063",
   "metadata": {},
   "source": [
    ">The preprocess_text function below **tokenizes** the input text, **lemmatizes and stems the tokens**, **removes stopwords** and **non-alphabetic** tokens. The function then **joins** the processed tokens back into a single string and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2079d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    word_tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "    # Lemmatize the tokens, stem the tokens, remove stopwords and non-alphabetic tokens\n",
    "    processed_tokens = [stemmer.stem(lemmatizer.lemmatize(w)) for w in word_tokens if w not in stop_words and w.isalpha()]\n",
    "\n",
    "    # Join the tokens back into a single string and return it\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5a4881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'tweet' column\n",
    "df['processed_tweet'] = df['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffefea90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>as woman complain clean hous man alway take trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>boy dat cold tyga dwn bad cuffin dat hoe place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>dawg you ever fuck bitch start cri you confus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>look like tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>the shit hear might true might faker bitch tol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "      <td>muthaf lie right hi tl trash now mine bibl scr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "      <td>gone broke wrong heart babi drove redneck crazi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "      <td>young buck wanna eat dat nigguh like aint fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "      <td>youu got wild bitch tellin lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "      <td>ruffl ntac eileen dahlia beauti color combin p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "0          3            0                   0        3      2   \n",
       "1          3            0                   3        0      1   \n",
       "2          3            0                   3        0      1   \n",
       "3          3            0                   2        1      1   \n",
       "4          6            0                   6        0      1   \n",
       "...      ...          ...                 ...      ...    ...   \n",
       "24778      3            0                   2        1      1   \n",
       "24779      3            0                   1        2      2   \n",
       "24780      3            0                   3        0      1   \n",
       "24781      6            0                   6        0      1   \n",
       "24782      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \\\n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "...                                                  ...   \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...   \n",
       "24779  you've gone and broke the wrong heart baby, an...   \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...   \n",
       "24781              youu got wild bitches tellin you lies   \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...   \n",
       "\n",
       "                                         processed_tweet  \n",
       "0      as woman complain clean hous man alway take trash  \n",
       "1         boy dat cold tyga dwn bad cuffin dat hoe place  \n",
       "2      dawg you ever fuck bitch start cri you confus ...  \n",
       "3                                       look like tranni  \n",
       "4      the shit hear might true might faker bitch tol...  \n",
       "...                                                  ...  \n",
       "24778  muthaf lie right hi tl trash now mine bibl scr...  \n",
       "24779    gone broke wrong heart babi drove redneck crazi  \n",
       "24780  young buck wanna eat dat nigguh like aint fuck...  \n",
       "24781                     youu got wild bitch tellin lie  \n",
       "24782  ruffl ntac eileen dahlia beauti color combin p...  \n",
       "\n",
       "[24783 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf49261",
   "metadata": {},
   "source": [
    "### train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48bf2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the text data\n",
    "X_processed = df['processed_tweet']\n",
    "y = df['class']\n",
    "\n",
    "X_train_processed, X_test_processed, y_train_processed, y_test_processed = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c1bda",
   "metadata": {},
   "source": [
    "# Advance Model 1\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### GridSearchCV - Bag Of Words, Tf-Idf and Hashing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d585a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression Count Vectorizer</th>\n",
       "      <td>{'count_vectorizer__max_features': None, 'count_vectorizer__min_df': 2, 'count_vectorizer__ngram_range': (1, 2), 'count_vectorizer__stop_words': None, 'logistic_regression__C': 0.1}</td>\n",
       "      <td>0.904267</td>\n",
       "      <td>296.870057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression Tfidf Vectorizer</th>\n",
       "      <td>{'logistic_regression__C': 10, 'tfidf_vectorizer__max_features': None, 'tfidf_vectorizer__min_df': 2, 'tfidf_vectorizer__ngram_range': (1, 2), 'tfidf_vectorizer__stop_words': None}</td>\n",
       "      <td>0.898366</td>\n",
       "      <td>291.928122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression Hashing Vectorizer</th>\n",
       "      <td>{'logistic_regression__C': 1}</td>\n",
       "      <td>0.905175</td>\n",
       "      <td>257.981519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                  best_params best_score    fit_time\n",
       "Logistic Regression Count Vectorizer    {'count_vectorizer__max_features': None, 'count_vectorizer__min_df': 2, 'count_vectorizer__ngram_range': (1, 2), 'count_vectorizer__stop_words': None, 'logistic_regression__C': 0.1}   0.904267  296.870057\n",
       "Logistic Regression Tfidf Vectorizer     {'logistic_regression__C': 10, 'tfidf_vectorizer__max_features': None, 'tfidf_vectorizer__min_df': 2, 'tfidf_vectorizer__ngram_range': (1, 2), 'tfidf_vectorizer__stop_words': None}   0.898366  291.928122\n",
       "Logistic Regression Hashing Vectorizer                                                                                                                                                          {'logistic_regression__C': 1}   0.905175  257.981519"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline with vectorizers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer())\n",
    "]\n",
    "\n",
    "# Only use Logistic Regression\n",
    "classifiers = [\n",
    "    ('logistic_regression', LogisticRegression(random_state=42))\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "        \n",
    "        if vectorizer_name != 'hashing_vectorizer':\n",
    "            param_grid = {\n",
    "                'logistic_regression__C': [0.1, 1, 10],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        else: # no params for HashingVectorizer\n",
    "            param_grid = {\n",
    "                'logistic_regression__C': [0.1, 1, 10]\n",
    "            }\n",
    "\n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e2857",
   "metadata": {},
   "source": [
    "# Advance Model 2\n",
    "\n",
    "## Decision Tree\n",
    "\n",
    "### GridSearchCV - Bag Of Words, Tf-Idf, Hashing and Binary vectorazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "573f7fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "      <th>fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree Count Vectorizer</th>\n",
       "      <td>{'count_vectorizer__max_features': None, 'count_vectorizer__min_df': 1, 'count_vectorizer__ngram_range': (1, 2), 'count_vectorizer__stop_words': None, 'decision_tree__max_depth': 15}</td>\n",
       "      <td>0.890346</td>\n",
       "      <td>125.677383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Tfidf Vectorizer</th>\n",
       "      <td>{'decision_tree__max_depth': 15, 'tfidf_vectorizer__max_features': None, 'tfidf_vectorizer__min_df': 2, 'tfidf_vectorizer__ngram_range': (1, 2), 'tfidf_vectorizer__stop_words': None}</td>\n",
       "      <td>0.889892</td>\n",
       "      <td>148.8977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Hashing Vectorizer</th>\n",
       "      <td>{'decision_tree__max_depth': 15, 'hashing_vectorizer__ngram_range': (1, 2), 'hashing_vectorizer__stop_words': None}</td>\n",
       "      <td>0.89085</td>\n",
       "      <td>93.428398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Binary Vectorizer</th>\n",
       "      <td>{'binary_vectorizer__max_features': None, 'binary_vectorizer__min_df': 1, 'binary_vectorizer__ngram_range': (1, 2), 'binary_vectorizer__stop_words': None, 'decision_tree__max_depth': 15}</td>\n",
       "      <td>0.891254</td>\n",
       "      <td>127.905169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                 best_params best_score    fit_time\n",
       "Decision Tree Count Vectorizer        {'count_vectorizer__max_features': None, 'count_vectorizer__min_df': 1, 'count_vectorizer__ngram_range': (1, 2), 'count_vectorizer__stop_words': None, 'decision_tree__max_depth': 15}   0.890346  125.677383\n",
       "Decision Tree Tfidf Vectorizer        {'decision_tree__max_depth': 15, 'tfidf_vectorizer__max_features': None, 'tfidf_vectorizer__min_df': 2, 'tfidf_vectorizer__ngram_range': (1, 2), 'tfidf_vectorizer__stop_words': None}   0.889892    148.8977\n",
       "Decision Tree Hashing Vectorizer                                                                         {'decision_tree__max_depth': 15, 'hashing_vectorizer__ngram_range': (1, 2), 'hashing_vectorizer__stop_words': None}    0.89085   93.428398\n",
       "Decision Tree Binary Vectorizer   {'binary_vectorizer__max_features': None, 'binary_vectorizer__min_df': 1, 'binary_vectorizer__ngram_range': (1, 2), 'binary_vectorizer__stop_words': None, 'decision_tree__max_depth': 15}   0.891254  127.905169"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pipeline with vectorizers and classifiers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer(norm=None)),\n",
    "    ('binary_vectorizer', CountVectorizer(binary=True))\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "\n",
    "        if classifier_name == 'decision_tree':\n",
    "            param_grid = {\n",
    "                'decision_tree__max_depth': [5, 10, 15],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english']\n",
    "            }\n",
    "            if vectorizer_name != 'hashing_vectorizer':\n",
    "                param_grid[f'{vectorizer_name}__min_df'] = [1, 2, 5]\n",
    "                param_grid[f'{vectorizer_name}__max_features'] = [None, 500, 1000]\n",
    "\n",
    "        \n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539834f1",
   "metadata": {},
   "source": [
    "# Advance Model 3\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "### GridSearchCV - Bag Of Words, Tf-Idf, Hashing and Binary vectorazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline with vectorizers and classifiers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer(norm=None)),\n",
    "    ('binary_vectorizer', CountVectorizer(binary=True))\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "\n",
    "        if classifier_name == 'random_forest':\n",
    "            param_grid = {\n",
    "                'random_forest__n_estimators': [100, 200, 300],\n",
    "                'random_forest__max_depth': [5, 10, 15],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "            if vectorizer_name != 'hashing_vectorizer':\n",
    "                param_grid[f'{vectorizer_name}__min_df'] = [1, 2, 5]\n",
    "                param_grid[f'{vectorizer_name}__max_features'] = [None, 500, 1000]\n",
    "\n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775e3e7",
   "metadata": {},
   "source": [
    "# Advance Model 4\n",
    "\n",
    "## Support Vector Machine\n",
    "\n",
    "### GridSearchCV - Bag Of Words, Tf-Idf, Hashing and Binary vectorazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37465d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with vectorizers and classifiers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer(norm=None)),\n",
    "    ('binary_vectorizer', CountVectorizer(binary=True))\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    ('svc', SVC(random_state=42))\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "\n",
    "        if classifier_name == 'svc':\n",
    "            param_grid = {\n",
    "                'svc__C': [0.1, 1, 10],\n",
    "                'svc__kernel': ['linear', 'rbf'],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "            if vectorizer_name != 'hashing_vectorizer':\n",
    "                param_grid[f'{vectorizer_name}__max_features'] = [None, 500, 1000]\n",
    "\n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42090a",
   "metadata": {},
   "source": [
    "# Advance Model 5\n",
    "\n",
    "## K Neigbors\n",
    "\n",
    "### GridSearchCV - Bag Of Words, Tf-Idf, Hashing and Binary vectorazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ea2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with vectorizers and classifiers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer(non_negative=True, norm=None)),\n",
    "    ('binary_vectorizer', CountVectorizer(binary=True))\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    ('k_neighbors', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "\n",
    "        if classifier_name == 'k_neighbors':\n",
    "            param_grid = {\n",
    "                'k_neighbors__n_neighbors': [3, 5, 7],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "            if vectorizer_name != 'hashing_vectorizer':\n",
    "                param_grid[f'{vectorizer_name}__max_features'] = [None, 500, 1000]\n",
    "        \n",
    "\n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e579d",
   "metadata": {},
   "source": [
    "# Advance Model 6\n",
    "\n",
    "## XGB\n",
    "\n",
    "### GridSearchCV - Bag Of Words, Tf-Idf, Hashing and Binary vectorazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee301976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with vectorizers and classifiers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer(non_negative=True, norm=None)),\n",
    "    ('binary_vectorizer', CountVectorizer(binary=True))\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(random_state=42)),\n",
    "    ('svc', SVC(random_state=42)),\n",
    "    ('k_neighbors', KNeighborsClassifier()),\n",
    "    ('xgb', XGBClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "\n",
    "        if classifier_name == 'decision_tree':\n",
    "            param_grid = {\n",
    "                'decision_tree__max_depth': [5, 10, 15],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'random_forest':\n",
    "            param_grid = {\n",
    "                'random_forest__n_estimators': [100, 200, 300],\n",
    "                'random_forest__max_depth': [5, 10, 15],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'svc':\n",
    "            param_grid = {\n",
    "                'svc__C': [0.1, 1, 10],\n",
    "                'svc__kernel': ['linear', 'rbf'],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'k_neighbors':\n",
    "            param_grid = {\n",
    "                'k_neighbors__n_neighbors': [3, 5, 7],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'xgb':\n",
    "            param_grid = {\n",
    "                'xgb__n_estimators': [100, 200, 300],\n",
    "                'xgb__max_depth': [3, 5, 7],\n",
    "                'xgb__learning_rate': [0.1, 0.01],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "\n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83db4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb257266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with vectorizers and classifiers\n",
    "vectorizers = [\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('tfidf_vectorizer', TfidfVectorizer()),\n",
    "    ('hashing_vectorizer', HashingVectorizer(non_negative=True, norm=None)),\n",
    "    ('binary_vectorizer', CountVectorizer(binary=True))\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('random_forest', RandomForestClassifier(random_state=42)),\n",
    "    ('svc', SVC(random_state=42)),\n",
    "    ('k_neighbors', KNeighborsClassifier()),\n",
    "    ('xgb', XGBClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers:\n",
    "    for classifier_name, classifier in classifiers:\n",
    "        pipe = Pipeline([\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (classifier_name, classifier)\n",
    "        ])\n",
    "\n",
    "        if classifier_name == 'decision_tree':\n",
    "            param_grid = {\n",
    "                'decision_tree__max_depth': [5, 10, 15],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'random_forest':\n",
    "            param_grid = {\n",
    "                'random_forest__n_estimators': [100, 200, 300],\n",
    "                'random_forest__max_depth': [5, 10, 15],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'svc':\n",
    "            param_grid = {\n",
    "                'svc__C': [0.1, 1, 10],\n",
    "                'svc__kernel': ['linear', 'rbf'],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'k_neighbors':\n",
    "            param_grid = {\n",
    "                'k_neighbors__n_neighbors': [3, 5, 7],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "        elif classifier_name == 'xgb':\n",
    "            param_grid = {\n",
    "                'xgb__n_estimators': [100, 200, 300],\n",
    "                'xgb__max_depth': [3, 5, 7],\n",
    "                'xgb__learning_rate': [0.1, 0.01],\n",
    "                f'{vectorizer_name}__ngram_range': [(1, 1), (1, 2)],\n",
    "                f'{vectorizer_name}__min_df': [1, 2, 5],\n",
    "                f'{vectorizer_name}__stop_words': [None, 'english'],\n",
    "                f'{vectorizer_name}__max_features': [None, 500, 1000]\n",
    "            }\n",
    "\n",
    "        model_key = f'{classifier_name.replace(\"_\", \" \").title()} {vectorizer_name.replace(\"_\", \" \").title()}'\n",
    "        model_results[model_key] = {'best_params': None, 'best_score': 0, 'fit_time': 0}\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, verbose=1)\n",
    "        start_time = time.time()\n",
    "        grid.fit(X_train_processed, y_train_processed)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if grid.best_score_ > model_results[model_key]['best_score']:\n",
    "            model_results[model_key]['best_params'] = grid.best_params_\n",
    "            model_results[model_key]['best_score'] = grid.best_score_\n",
    "            model_results[model_key]['fit_time'] = elapsed_time\n",
    "\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
